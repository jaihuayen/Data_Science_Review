\frametitle{CBOW}

  \begin{center}
    \includegraphics[scale=0.3]{../images/img_5.png} \\  \href{https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/readings/cs224n-2019-notes01-wordvecs1.pdf}{[Image Source]}
  \end{center}

  \begin{itemize}
    \item Use probability $P(y_{i} | x_{1k}, x_{2k}, ... , x_{Ck})$ to learn the weight matrix $W$!
    \item $W$ is used to be the pre-trained model when we transform the words into embedding vectors in the unseen documents.
  \end{itemize}

